{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "UpMHkIhmWTYH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame from the provided CSV\n",
        "df = pd.read_csv('/content/feature_extracted_data.csv')\n",
        "print(\"Column Names:\", df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g76qjMYqatEV",
        "outputId": "dcd76f0e-c514-475e-ee8d-b93a2594dbea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Names: Index(['Word', 'Initial Scopes', 'Final Scopes', 'Tag', 'POS', 'LEMMA',\n",
            "       'NUMBER', 'Contains NUMBER', 'Maj NUMBER', 'text_id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaQcxotWNvj",
        "outputId": "3157d124-3b8d-4b99-cc63-c7149740bcfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Columns:        Word  Word_idx    POS  POS_idx     LEMMA  Lemma_idx Tag  Tag_idx\n",
            "0  paciente     21226   NOUN        7  paciente      10546   O        2\n",
            "1       que     23115  SCONJ       14       que      11595   O        2\n",
            "2   ingresa     17572   VERB       16  ingresar       8590   O        2\n",
            "3        de     12698    ADP        1        de       5845   O        2\n",
            "4     forma     15630   NOUN        7     forma       7489   O        2\n"
          ]
        }
      ],
      "source": [
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df['Word_idx'] = word_encoder.fit_transform(df['Word'])\n",
        "df['POS_idx'] = pos_encoder.fit_transform(df['POS'])\n",
        "df['Lemma_idx'] = lemma_encoder.fit_transform(df['LEMMA'])\n",
        "df['Tag_idx'] = tag_encoder.fit_transform(df['Tag'])\n",
        "\n",
        "\n",
        "\n",
        "# Verify new columns\n",
        "print(\"Encoded Columns:\", df[['Word', 'Word_idx', 'POS', 'POS_idx', 'LEMMA', 'Lemma_idx', 'Tag', 'Tag_idx']].head())\n",
        "\n",
        "\n",
        "# Custom Dataset class\n",
        "class NLPCustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NLPModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim):\n",
        "        super(NLPModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)  # Pre-trained, frozen\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.lemma_embedding = nn.Embedding(num_embeddings_lemma, lemma_embedding_dim)\n",
        "\n",
        "        # Concatenation dimension\n",
        "        concat_dim = embedding_dim + pos_embedding_dim + lemma_embedding_dim\n",
        "\n",
        "        # Dense layer prior to LSTM\n",
        "        self.dense = nn.Linear(concat_dim, hidden_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_out_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(2 * lstm_out_dim, output_dim)  # Correct input dimension\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x, pos, lemma):\n",
        "        # Embed each input type\n",
        "        x = self.word_embeddings(x)\n",
        "        pos = self.pos_embeddings(pos)\n",
        "        lemma = self.lemma_embedding(lemma)  # Corrected from 'case' to 'lemma'\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat((x, pos, lemma), dim=-1)\n",
        "\n",
        "        # Apply dense and activation\n",
        "        x = torch.tanh(self.dense(x))\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(lstm_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "8F8TFkvnWaqf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating Dataset and DataLoader for training and validation\n",
        "train_dataset = NLPCustomDataset(train_df)\n",
        "val_dataset = NLPCustomDataset(val_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initializing the model\n",
        "model = NLPModel(\n",
        "    num_embeddings=len(word_encoder.classes_),\n",
        "    embedding_dim=50,\n",
        "    num_pos_tags=len(pos_encoder.classes_),\n",
        "    pos_embedding_dim=10,\n",
        "    num_embeddings_lemma=len(lemma_encoder.classes_),\n",
        "    lemma_embedding_dim=10,\n",
        "    hidden_dim=100,\n",
        "    lstm_out_dim=50,\n",
        "    output_dim=len(tag_encoder.classes_)\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPjyYXB_YZ3d",
        "outputId": "f7e191ce-a6ad-44af-ffa5-57e41567a953"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLPModel(\n",
            "  (word_embeddings): Embedding(27100, 50)\n",
            "  (pos_embeddings): Embedding(18, 10)\n",
            "  (lemma_embedding): Embedding(13927, 10)\n",
            "  (dense): Linear(in_features=70, out_features=100, bias=True)\n",
            "  (lstm): LSTM(100, 50, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "   # Validation step (optional)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            words = batch['word']\n",
        "            pos = batch['pos']\n",
        "            lemma = batch['lemma']\n",
        "            tags = batch['tag']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(words, pos, lemma)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, -1)\n",
        "            correct_predictions += (predicted == tags).sum().item()\n",
        "            total_predictions += tags.numel()\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy().flatten())\n",
        "            all_val_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='weighted')\n",
        "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "    print(f'Validation Precision: {val_precision}, Recall: {val_recall}, F1 Score: {val_f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7hA2VjoXpmQ",
        "outputId": "633dbe01-8752-4c47-f699-45b6f8a6c497"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.3336250530734478\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.2770901247294563, Validation Accuracy: 0.9228195867132453\n",
            "Validation Precision: 0.9050709179037977, Recall: 0.9228195867132453, F1 Score: 0.9022250007940289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST"
      ],
      "metadata": {
        "id": "jkY-k_iKllrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test CSV file\n",
        "test_file_path = '/content/feature_extracted_dataTEST.csv'\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "# Encode the test data using the existing label encoders\n",
        "df_test['Word_idx'] = word_encoder.transform(df_test['Word'])\n",
        "df_test['POS_idx'] = pos_encoder.transform(df_test['POS'])\n",
        "df_test['Lemma_idx'] = lemma_encoder.transform(df_test['LEMMA'])\n",
        "\n",
        "# Add a special token for unknown tags in the encoder\n",
        "if 'unknown' not in tag_encoder.classes_:\n",
        "    tag_encoder.classes_ = list(tag_encoder.classes_) + ['unknown']\n",
        "\n",
        "# Manually map tags to their indices, using 'unknown' for unseen tags\n",
        "def map_tags(tag):\n",
        "    try:\n",
        "        return tag_encoder.transform([tag])[0]\n",
        "    except ValueError:\n",
        "        return tag_encoder.transform(['unknown'])[0]\n",
        "\n",
        "df_test['Tag_idx'] = df_test['Tag'].apply(map_tags)\n",
        "\n",
        "# Verify the encoded columns\n",
        "print(\"Encoded Columns TEST:\", df_test[['Word', 'Word_idx', 'POS', 'POS_idx', 'LEMMA', 'Lemma_idx', 'Tag', 'Tag_idx']].head())\n",
        "\n",
        "# Custom Dataset class for test data\n",
        "class NLPCustomTestDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Create Dataset and DataLoader for test data\n",
        "test_dataset = NLPCustomTestDataset(df_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "_0MCWcpjlnS-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "184478ea-6eea-4791-94bf-5f2dbc1c258c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y contains previously unseen labels: 'antcededentes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'antcededentes'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-74f5bed55a61>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Encode the test data using the existing label encoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lemma_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LEMMA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'antcededentes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, -1)\n",
        "        correct_predictions += (predicted == tags).sum().item()\n",
        "        total_predictions += tags.numel()\n",
        "\n",
        "        all_test_preds.extend(predicted.cpu().numpy().flatten())\n",
        "        all_test_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_dataloader)\n",
        "test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(all_test_labels, all_test_preds, average='weighted')\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}')\n",
        "print(f'Test Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}')"
      ],
      "metadata": {
        "id": "UNmtmza9n6vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying things area"
      ],
      "metadata": {
        "id": "BVgZQ-5z5Lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df['Word_idx'] = word_encoder.fit_transform(df['Word'])\n",
        "df['POS_idx'] = pos_encoder.fit_transform(df['POS'])\n",
        "df['Lemma_idx'] = lemma_encoder.fit_transform(df['LEMMA'])\n",
        "df['Tag_idx'] = tag_encoder.fit_transform(df['Tag'])\n",
        "\n",
        "# Add 'unknown' index for words and lemmas\n",
        "def add_unknown_to_encoder(encoder, name):\n",
        "    classes = list(encoder.classes_)\n",
        "    if 'unknown' not in classes:\n",
        "        classes.append('unknown')\n",
        "        encoder.classes_ = np.array(classes)\n",
        "    return encoder\n",
        "\n",
        "word_encoder = add_unknown_to_encoder(word_encoder, 'Word')\n",
        "lemma_encoder = add_unknown_to_encoder(lemma_encoder, 'LEMMA')\n",
        "\n",
        "# Function to map words/lemmas/tags to their indices, using 'unknown' for unseen labels\n",
        "def map_to_index(encoder, items, unknown_label='unknown'):\n",
        "    label_map = {label: idx for idx, label in enumerate(encoder.classes_)}\n",
        "    return [label_map.get(item, label_map[unknown_label]) for item in items]\n",
        "\n",
        "df['Word_idx'] = map_to_index(word_encoder, df['Word'])\n",
        "df['Lemma_idx'] = map_to_index(lemma_encoder, df['LEMMA'])\n",
        "df['Tag_idx'] = map_to_index(tag_encoder, df['Tag'])\n",
        "\n",
        "# Custom Dataset class\n",
        "class NLPCustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating Dataset and DataLoader for training and validation\n",
        "train_dataset = NLPCustomDataset(train_df)\n",
        "val_dataset = NLPCustomDataset(val_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initializing the model\n",
        "model = NLPModel(\n",
        "    num_embeddings=len(word_encoder.classes_),\n",
        "    embedding_dim=50,\n",
        "    num_pos_tags=len(pos_encoder.classes_),\n",
        "    pos_embedding_dim=10,\n",
        "    num_embeddings_lemma=len(lemma_encoder.classes_),\n",
        "    lemma_embedding_dim=10,\n",
        "    hidden_dim=100,\n",
        "    lstm_out_dim=50,\n",
        "    output_dim=len(tag_encoder.classes_)\n",
        ")\n",
        "\n",
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, -1)\n",
        "        correct_predictions += (predicted == tags).sum().item()\n",
        "        total_predictions += tags.numel()\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy().flatten())\n",
        "        all_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Training Accuracy: {accuracy}')\n",
        "    print(f'Training Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
        "\n",
        "    # Validation step (optional)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            words = batch['word']\n",
        "            pos = batch['pos']\n",
        "            lemma = batch['lemma']\n",
        "            tags = batch['tag']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(words, pos, lemma)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, -1)\n",
        "            correct_predictions += (predicted == tags).sum().item()\n",
        "            total_predictions += tags.numel()\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy().flatten())\n",
        "            all_val_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='weighted')\n",
        "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "    print(f'Validation Precision: {val_precision}, Recall: {val_recall}, F1 Score: {val_f1}')\n",
        "\n",
        "# Load the test CSV file\n",
        "test_file_path = '/mnt/data/feature_extracted_dataTEST.csv'\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "\n",
        "# Encode the test data using the existing label encoders\n",
        "df_test['Word_idx'] = map_to_index(word_encoder, df_test['Word'])\n",
        "df_test['POS_idx'] = map_to_index(pos_encoder, df_test['POS'])\n",
        "df_test['Lemma_idx'] = map_to_index(lemma_encoder, df_test['LEMMA'])\n",
        "df_test['Tag_idx'] = map_to_index(tag_encoder, df_test['Tag'])\n",
        "\n",
        "# Verify the encoded columns\n",
        "print(\"Encoded Columns TEST:\", df_test[['Word', 'Word_idx', 'POS', 'POS_idx', 'LEMMA', 'Lemma_idx', 'Tag', 'Tag_idx']].head())\n",
        "\n",
        "# Custom Dataset class for test data\n",
        "class NLPCustomTestDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Create Dataset and DataLoader for test data\n",
        "test_dataset = NLPCustomTestDataset(df_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, -1)\n",
        "        correct_predictions += (predicted == tags).sum().item()\n",
        "        total_predictions += tags.numel()\n",
        "\n",
        "        all_test_preds.extend(predicted.cpu().numpy().flatten())\n",
        "        all_test_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_dataloader)\n",
        "test_accuracy = correct\n"
      ],
      "metadata": {
        "id": "xxI5lXU-5OMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}