{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Loading libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HFIdEW8oJZUi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TODO:**\n",
        "- include casing features\n",
        "- fix the all Os problem\n",
        "- more plots -> conf matrix\n",
        "\n",
        "\n",
        "**mistakes:**\n",
        "- overfitting -> regularization\n",
        "- all Os problem\n",
        "\n",
        "\n",
        "**things to try:**\n",
        "- seaborn sns plots\n",
        "- regularization\n",
        "- adjust model parameters\n",
        "- adjust model in itself\n",
        "- batch normalization\n",
        "- SMOT\n",
        "- class weights"
      ],
      "metadata": {
        "id": "6cxHV2XhLw79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "DQkeuwhDJmTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class NLPModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim):\n",
        "        super(NLPModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)  # Pre-trained, frozen\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.lemma_embedding = nn.Embedding(num_embeddings_lemma, lemma_embedding_dim)\n",
        "\n",
        "        # Concatenation dimension\n",
        "        concat_dim = embedding_dim + pos_embedding_dim + lemma_embedding_dim\n",
        "\n",
        "        # Dense layer prior to LSTM\n",
        "        self.dense = nn.Linear(concat_dim, hidden_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_out_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(2 * lstm_out_dim, output_dim)  # Correct input dimension\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x, pos, lemma):\n",
        "        # Embed each input type\n",
        "        x = self.word_embeddings(x)\n",
        "        pos = self.pos_embeddings(pos)\n",
        "        lemma = self.lemma_embedding(lemma)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat((x, pos, lemma), dim=-1)\n",
        "\n",
        "        # Apply dense and activation\n",
        "        x = torch.tanh(self.dense(x))\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(lstm_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "M2UNmM1MJtE0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "oYx5NxsnJuNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloading"
      ],
      "metadata": {
        "id": "HEzl8p-oJyBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the CSV file\n",
        "file_path = '/content/feature_extracted_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df['Word_idx'] = word_encoder.fit_transform(df['Word'])\n",
        "df['POS_idx'] = pos_encoder.fit_transform(df['POS'])\n",
        "df['Lemma_idx'] = lemma_encoder.fit_transform(df['LEMMA'])\n",
        "df['Tag_idx'] = tag_encoder.fit_transform(df['Tag'])\n",
        "\n",
        "# Add 'unknown' index for words, lemmas, and tags\n",
        "def add_unknown_to_encoder(encoder):\n",
        "    classes = list(encoder.classes_)\n",
        "    if 'unknown' not in classes:\n",
        "        classes.append('unknown')\n",
        "        encoder.classes_ = np.array(classes)\n",
        "    return encoder\n",
        "\n",
        "word_encoder = add_unknown_to_encoder(word_encoder)\n",
        "lemma_encoder = add_unknown_to_encoder(lemma_encoder)\n",
        "tag_encoder = add_unknown_to_encoder(tag_encoder)\n",
        "\n",
        "# Function to map words/lemmas/tags to their indices, using 'unknown' for unseen labels\n",
        "def map_to_index(encoder, items, unknown_label='unknown'):\n",
        "    label_map = {label: idx for idx, label in enumerate(encoder.classes_)}\n",
        "    return [label_map.get(item, label_map[unknown_label]) for item in items]\n",
        "\n",
        "df['Word_idx'] = map_to_index(word_encoder, df['Word'])\n",
        "df['Lemma_idx'] = map_to_index(lemma_encoder, df['LEMMA'])\n",
        "df['Tag_idx'] = map_to_index(tag_encoder, df['Tag'])"
      ],
      "metadata": {
        "id": "sBtSmtM4JdSI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class\n",
        "class NLPCustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating Dataset and DataLoader for training and validation\n",
        "train_dataset = NLPCustomDataset(train_df)\n",
        "val_dataset = NLPCustomDataset(val_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "40YOn-9dJiL8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter initialization"
      ],
      "metadata": {
        "id": "aflpNfJ3J5fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model = NLPModel(\n",
        "    num_embeddings=len(word_encoder.classes_),\n",
        "    embedding_dim=50,\n",
        "    num_pos_tags=len(pos_encoder.classes_),\n",
        "    pos_embedding_dim=10,\n",
        "    num_embeddings_lemma=len(lemma_encoder.classes_),\n",
        "    lemma_embedding_dim=10,\n",
        "    hidden_dim=100,\n",
        "    lstm_out_dim=50,\n",
        "    output_dim=len(tag_encoder.classes_)\n",
        ")\n",
        "\n",
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "IAPgwj0LJ_Xq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trianing Loop"
      ],
      "metadata": {
        "id": "WBaV8RxcKG23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "1QujZiNmIyu5",
        "outputId": "13bdcc89-eaef-47f5-8039-44b36d337495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3335518239314328\n",
            "Validation Loss: 0.28256620940131444, Validation Accuracy: 0.9228195867132453\n",
            "Validation Precision: 0.9056098150795748, Recall: 0.9228195867132453, F1 Score: 0.901674095434056\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7451c18200d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-028b23efb904>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, pos, lemma)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Embed each input type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_buffers'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f'\\nEpoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "    # Validation step (optional)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            words = batch['word']\n",
        "            pos = batch['pos']\n",
        "            lemma = batch['lemma']\n",
        "            tags = batch['tag']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(words, pos, lemma)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, -1)\n",
        "            correct_predictions += (predicted == tags).sum().item()\n",
        "            total_predictions += tags.numel()\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy().flatten())\n",
        "            all_val_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='weighted',zero_division=0)\n",
        "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "    print(f'Validation Precision: {val_precision}, Recall: {val_recall}, F1 Score: {val_f1}')\n",
        "\n",
        "# + metricas: en que me equivoco que clase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ctAiA3FfKLGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some example predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(100):\n",
        "    true_label = tag_encoder.inverse_transform([all_val_labels[i]])[0]\n",
        "    predicted_label = tag_encoder.inverse_transform([all_val_preds[i]])[0]\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "VBKbn0ZIKN55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "hFIL3vz-KRuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloading Test Set"
      ],
      "metadata": {
        "id": "-7O9C91vKa4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test CSV file\n",
        "test_file_path = '/content/feature_extracted_dataTEST.csv'\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "\n",
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df_test['Word_idx'] = word_encoder.fit_transform(df_test['Word'])\n",
        "df_test['POS_idx'] = pos_encoder.fit_transform(df_test['POS'])\n",
        "df_test['Lemma_idx'] = lemma_encoder.fit_transform(df_test['LEMMA'])\n",
        "df_test['Tag_idx'] = tag_encoder.fit_transform(df_test['Tag'])\n",
        "\n",
        "word_encoder = add_unknown_to_encoder(word_encoder)\n",
        "lemma_encoder = add_unknown_to_encoder(lemma_encoder)\n",
        "tag_encoder = add_unknown_to_encoder(tag_encoder)\n",
        "\n",
        "df_test['Word_idx'] = map_to_index(word_encoder, df_test['Word'])\n",
        "df_test['Lemma_idx'] = map_to_index(lemma_encoder, df_test['LEMMA'])\n",
        "df_test['Tag_idx'] = map_to_index(tag_encoder, df_test['Tag'])\n",
        "\n",
        "\n",
        "# Custom Dataset class for test data\n",
        "class NLPCustomTestDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Create Dataset and DataLoader for test data\n",
        "test_dataset = NLPCustomTestDataset(df_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "zEO-tZ8vKaQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "HPEdKTfbKidy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, -1)\n",
        "        correct_predictions += (predicted == tags).sum().item()\n",
        "        total_predictions += tags.numel()\n",
        "\n",
        "        all_test_preds.extend(predicted.cpu().numpy().flatten())\n",
        "        all_test_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_dataloader)\n",
        "test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}')\n",
        "print(f'Test Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}')"
      ],
      "metadata": {
        "id": "WaQ1UXWYKT5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some example predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(1000):\n",
        "    true_label = tag_encoder.inverse_transform([all_test_labels[i]])[0]\n",
        "    predicted_label = tag_encoder.inverse_transform([all_test_preds[i]])[0]\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "gKS2SHELKWrE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}