{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Loading libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "HFIdEW8oJZUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TODO:**\n",
        "- include casing features\n",
        "- fix the all Os problem\n",
        "- more plots -> conf matrix\n",
        "- look for new metrics\n",
        "\n",
        "\n",
        "**mistakes:**\n",
        "- overfitting -> regularization\n",
        "- all Os problem\n",
        "\n",
        "\n",
        "**things to try:**\n",
        "- seaborn sns plots\n",
        "- regularization\n",
        "- adjust model parameters\n",
        "- adjust model in itself\n",
        "- batch normalization\n",
        "- SMOT\n",
        "- class weights"
      ],
      "metadata": {
        "id": "6cxHV2XhLw79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "DQkeuwhDJmTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class NLPModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim):\n",
        "        super(NLPModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)  # Pre-trained, frozen\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.lemma_embedding = nn.Embedding(num_embeddings_lemma, lemma_embedding_dim)\n",
        "\n",
        "        # Concatenation dimension\n",
        "        concat_dim = embedding_dim + pos_embedding_dim + lemma_embedding_dim\n",
        "\n",
        "        # Dense layer prior to LSTM\n",
        "        self.dense = nn.Linear(concat_dim, hidden_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_out_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(2 * lstm_out_dim, output_dim)  # Correct input dimension\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x, pos, lemma):\n",
        "        # Embed each input type\n",
        "        x = self.word_embeddings(x)\n",
        "        pos = self.pos_embeddings(pos)\n",
        "        lemma = self.lemma_embedding(lemma)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat((x, pos, lemma), dim=-1)\n",
        "\n",
        "        # Apply dense and activation\n",
        "        x = torch.tanh(self.dense(x))\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(lstm_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "M2UNmM1MJtE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloading"
      ],
      "metadata": {
        "id": "HEzl8p-oJyBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the CSV file\n",
        "file_path = '/content/feature_extracted_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df['Word_idx'] = word_encoder.fit_transform(df['Word'])\n",
        "df['POS_idx'] = pos_encoder.fit_transform(df['POS'])\n",
        "df['Lemma_idx'] = lemma_encoder.fit_transform(df['LEMMA'])\n",
        "df['Tag_idx'] = tag_encoder.fit_transform(df['Tag'])\n",
        "\n",
        "# Add 'unknown' index for words, lemmas, and tags\n",
        "def add_unknown_to_encoder(encoder):\n",
        "    classes = list(encoder.classes_)\n",
        "    if 'unknown' not in classes:\n",
        "        classes.append('unknown')\n",
        "        encoder.classes_ = np.array(classes)\n",
        "    return encoder\n",
        "\n",
        "word_encoder = add_unknown_to_encoder(word_encoder)\n",
        "lemma_encoder = add_unknown_to_encoder(lemma_encoder)\n",
        "tag_encoder = add_unknown_to_encoder(tag_encoder)\n",
        "\n",
        "# Function to map words/lemmas/tags to their indices, using 'unknown' for unseen labels\n",
        "def map_to_index(encoder, items, unknown_label='unknown'):\n",
        "    label_map = {label: idx for idx, label in enumerate(encoder.classes_)}\n",
        "    return [label_map.get(item, label_map[unknown_label]) for item in items]\n",
        "\n",
        "df['Word_idx'] = map_to_index(word_encoder, df['Word'])\n",
        "df['Lemma_idx'] = map_to_index(lemma_encoder, df['LEMMA'])\n",
        "df['Tag_idx'] = map_to_index(tag_encoder, df['Tag'])"
      ],
      "metadata": {
        "id": "sBtSmtM4JdSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class\n",
        "class NLPCustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating Dataset and DataLoader for training and validation\n",
        "train_dataset = NLPCustomDataset(train_df)\n",
        "val_dataset = NLPCustomDataset(val_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "40YOn-9dJiL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter initialization"
      ],
      "metadata": {
        "id": "aflpNfJ3J5fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model = NLPModel(\n",
        "    num_embeddings=len(word_encoder.classes_),\n",
        "    embedding_dim=50,\n",
        "    num_pos_tags=len(pos_encoder.classes_),\n",
        "    pos_embedding_dim=10,\n",
        "    num_embeddings_lemma=len(lemma_encoder.classes_),\n",
        "    lemma_embedding_dim=10,\n",
        "    hidden_dim=100,\n",
        "    lstm_out_dim=50,\n",
        "    output_dim=len(tag_encoder.classes_)\n",
        ")\n",
        "\n",
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "IAPgwj0LJ_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trianing Loop"
      ],
      "metadata": {
        "id": "WBaV8RxcKG23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QujZiNmIyu5",
        "outputId": "e71c2a4c-eda6-43f0-f4a1-2295adbc3352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10, Loss: 0.33961817703871205\n",
            "Validation Loss: 0.2830888888283605, Validation Accuracy: 0.9219610397300018\n",
            "Validation Precision: 0.9058248618049888, Recall: 0.9219610397300018, F1 Score: 0.8987847086965588\n",
            "\n",
            "Epoch 2/10, Loss: 0.26978799859434544\n",
            "Validation Loss: 0.26071298103709, Validation Accuracy: 0.9267866658771982\n",
            "Validation Precision: 0.9101412085555345, Recall: 0.9267866658771982, F1 Score: 0.9087818498924212\n",
            "\n",
            "Epoch 3/10, Loss: 0.243847495203398\n",
            "Validation Loss: 0.2537777282784439, Validation Accuracy: 0.9293327017585411\n",
            "Validation Precision: 0.913810326619613, Recall: 0.9293327017585411, F1 Score: 0.9129347938201985\n",
            "\n",
            "Epoch 4/10, Loss: 0.2288687646026447\n",
            "Validation Loss: 0.25366043979231495, Validation Accuracy: 0.9277636331339926\n",
            "Validation Precision: 0.9149763806017059, Recall: 0.9277636331339926, F1 Score: 0.9149609615679594\n",
            "\n",
            "Epoch 5/10, Loss: 0.2169833548314977\n",
            "Validation Loss: 0.2581630850944555, Validation Accuracy: 0.9244478654745693\n",
            "Validation Precision: 0.911949108579328, Recall: 0.9244478654745693, F1 Score: 0.9146182943128681\n",
            "\n",
            "Epoch 6/10, Loss: 0.20823458226835617\n",
            "Validation Loss: 0.2547533105843644, Validation Accuracy: 0.9267570608088105\n",
            "Validation Precision: 0.9141663498719038, Recall: 0.9267570608088105, F1 Score: 0.9156163342610297\n",
            "\n",
            "Epoch 7/10, Loss: 0.2010034359100586\n",
            "Validation Loss: 0.25700106146695995, Validation Accuracy: 0.9243590502694061\n",
            "Validation Precision: 0.9142401734732821, Recall: 0.9243590502694061, F1 Score: 0.9147801656251581\n",
            "\n",
            "Epoch 8/10, Loss: 0.19503328095563152\n",
            "Validation Loss: 0.26371178730517963, Validation Accuracy: 0.9200663153531885\n",
            "Validation Precision: 0.9087389332223937, Recall: 0.9200663153531885, F1 Score: 0.9126399493207615\n",
            "\n",
            "Epoch 9/10, Loss: 0.190457349302604\n",
            "Validation Loss: 0.2665509355296804, Validation Accuracy: 0.9215465687725739\n",
            "Validation Precision: 0.9085498597085887, Recall: 0.9215465687725739, F1 Score: 0.9131923389269228\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f'\\nEpoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "    # Validation step (optional)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            words = batch['word']\n",
        "            pos = batch['pos']\n",
        "            lemma = batch['lemma']\n",
        "            tags = batch['tag']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(words, pos, lemma)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, -1)\n",
        "            correct_predictions += (predicted == tags).sum().item()\n",
        "            total_predictions += tags.numel()\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy().flatten())\n",
        "            all_val_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='weighted',zero_division=0)\n",
        "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "    print(f'Validation Precision: {val_precision}, Recall: {val_recall}, F1 Score: {val_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(all_val_labels, all_val_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm')\n",
        "plt.xlabel('Val Predicted Labels')\n",
        "plt.ylabel('Val True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cDnLequy46ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ctAiA3FfKLGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some example predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(100):\n",
        "    true_label = tag_encoder.inverse_transform([all_val_labels[i]])[0]\n",
        "    predicted_label = tag_encoder.inverse_transform([all_val_preds[i]])[0]\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "VBKbn0ZIKN55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "hFIL3vz-KRuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloading Test Set"
      ],
      "metadata": {
        "id": "-7O9C91vKa4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test CSV file\n",
        "test_file_path = '/content/feature_extracted_dataTEST.csv'\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "\n",
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df_test['Word_idx'] = word_encoder.fit_transform(df_test['Word'])\n",
        "df_test['POS_idx'] = pos_encoder.fit_transform(df_test['POS'])\n",
        "df_test['Lemma_idx'] = lemma_encoder.fit_transform(df_test['LEMMA'])\n",
        "df_test['Tag_idx'] = tag_encoder.fit_transform(df_test['Tag'])\n",
        "\n",
        "word_encoder = add_unknown_to_encoder(word_encoder)\n",
        "lemma_encoder = add_unknown_to_encoder(lemma_encoder)\n",
        "tag_encoder = add_unknown_to_encoder(tag_encoder)\n",
        "\n",
        "df_test['Word_idx'] = map_to_index(word_encoder, df_test['Word'])\n",
        "df_test['Lemma_idx'] = map_to_index(lemma_encoder, df_test['LEMMA'])\n",
        "df_test['Tag_idx'] = map_to_index(tag_encoder, df_test['Tag'])\n",
        "\n",
        "\n",
        "# Custom Dataset class for test data\n",
        "class NLPCustomTestDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Create Dataset and DataLoader for test data\n",
        "test_dataset = NLPCustomTestDataset(df_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "zEO-tZ8vKaQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "HPEdKTfbKidy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, -1)\n",
        "        correct_predictions += (predicted == tags).sum().item()\n",
        "        total_predictions += tags.numel()\n",
        "\n",
        "        all_test_preds.extend(predicted.cpu().numpy().flatten())\n",
        "        all_test_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_dataloader)\n",
        "test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}')\n",
        "print(f'Test Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}')"
      ],
      "metadata": {
        "id": "WaQ1UXWYKT5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(all_test_labels, all_test_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
        "plt.xlabel('Test Predicted Labels')\n",
        "plt.ylabel('Test True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rjBtet-A5eFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some example predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(100):\n",
        "    true_label = tag_encoder.inverse_transform([all_test_labels[i]])[0]\n",
        "    predicted_label = tag_encoder.inverse_transform([all_test_preds[i]])[0]\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "gKS2SHELKWrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revised Model\n",
        "\n",
        "Including\n",
        "- Regularization\n",
        "- Batch Normalization\n",
        "- Class weights\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPpG3hhB6Wmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model - Revised"
      ],
      "metadata": {
        "id": "kTBTBeAu7kaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLPModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim):\n",
        "        super(NLPModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)  # Pre-trained, frozen\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.lemma_embedding = nn.Embedding(num_embeddings_lemma, lemma_embedding_dim)\n",
        "\n",
        "        # Concatenation dimension\n",
        "        concat_dim = embedding_dim + pos_embedding_dim + lemma_embedding_dim\n",
        "\n",
        "        # Dense layer prior to LSTM\n",
        "        self.dense = nn.Linear(concat_dim, hidden_dim)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(concat_dim)  # Batch normalization\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_out_dim, batch_first=True, bidirectional=True)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(2 * lstm_out_dim)  # Batch normalization\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(2 * lstm_out_dim, output_dim)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)  # Increased dropout for regularization\n",
        "\n",
        "    def forward(self, x, pos, lemma):\n",
        "        # Embed each input type\n",
        "        x = self.word_embeddings(x)\n",
        "        pos = self.pos_embeddings(pos)\n",
        "        lemma = self.lemma_embedding(lemma)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat((x, pos, lemma), dim=-1)\n",
        "\n",
        "        # Apply batch normalization and dense layer\n",
        "        x = self.batch_norm1(x)\n",
        "        x = torch.tanh(self.dense(x))\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Apply batch normalization after LSTM\n",
        "        lstm_out = self.batch_norm2(lstm_out)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(lstm_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "8_BxZHp17nde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revised Data Loading"
      ],
      "metadata": {
        "id": "jgW71llX8a26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and preprocess\n",
        "train_data = pd.read_csv('/content/feature_extracted_data.csv')\n",
        "test_data = pd.read_csv('/content/feature_extracted_dataTEST.csv')\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "train_data['Tag'] = le.fit_transform(train_data['Tag'])\n",
        "test_data['Tag'] = le.transform(test_data['Tag'])\n",
        "\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        return {\n",
        "            'word': torch.tensor(item['Word'], dtype=torch.long),\n",
        "            'pos': torch.tensor(item['POS'], dtype=torch.long),\n",
        "            'lemma': torch.tensor(item['LEMMA'], dtype=torch.long),\n",
        "            'label': torch.tensor(item['Tag'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(train_data)\n",
        "val_dataset = CustomDataset(val_data)\n",
        "test_dataset = CustomDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "i9pSu87r8agS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Tag', data=train_data)\n",
        "plt.title('Class Distribution in Training Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yz1jrHsv9PWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, criterion, and optimizer\n",
        "num_embeddings = train_data['Word'].nunique()\n",
        "embedding_dim = 100\n",
        "num_pos_tags = train_data['POS'].nunique()\n",
        "pos_embedding_dim = 25\n",
        "num_embeddings_lemma = train_data['LEMMA'].nunique()\n",
        "lemma_embedding_dim = 25\n",
        "hidden_dim = 128\n",
        "lstm_out_dim = 64\n",
        "output_dim = len(le.classes_)\n",
        "\n",
        "\n",
        "model = NLPModel(num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim)\n",
        "\n",
        "class_weights = torch.tensor([0.1, 0.9], dtype=torch.float)  # Adjust class weights based on distribution\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9DbE9uT68vvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revised Training Loop"
      ],
      "metadata": {
        "id": "SZjuv6oy8Mk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch['word'], batch['pos'], batch['lemma'])\n",
        "        loss = criterion(outputs.view(-1, output_dim), batch['label'].view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            outputs = model(batch['word'], batch['pos'], batch['lemma'])\n",
        "            loss = criterion(outputs.view(-1, output_dim), batch['label'].view(-1))\n",
        "            val_running_loss += loss.item()\n",
        "\n",
        "    val_loss = val_running_loss / len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')"
      ],
      "metadata": {
        "id": "VQLzly7Z8EQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            outputs = model(batch['word'], batch['pos'], batch['lemma'])\n",
        "            preds = torch.argmax(outputs, dim=2)\n",
        "            all_preds.extend(preds.view(-1).numpy())\n",
        "            all_labels.extend(batch['label'].view(-1).numpy())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "5bZF9zlx-9Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation set\n",
        "val_precision, val_recall, val_f1, val_accuracy, val_conf_matrix, val_labels, val_preds = evaluate(val_loader)\n",
        "print(f'Validation Precision: {val_precision}, Recall: {val_recall}, F1-Score: {val_f1}, Accuracy: {val_accuracy}')"
      ],
      "metadata": {
        "id": "XaYT0m-z-5kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting"
      ],
      "metadata": {
        "id": "PBPd7Pc6_PkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix for validation set\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='coolwarm')\n",
        "plt.xlabel('Validation Predicted Labels')\n",
        "plt.ylabel('Validation True Labels')\n",
        "plt.title('Validation Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qb46dmwB_BqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2p5YwBb6_HN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some example predictions for validation set\n",
        "print(\"\\nSample Predictions for Validation Set:\")\n",
        "for i in range(100):\n",
        "    true_label = le.inverse_transform([val_labels[i]])[0]\n",
        "    predicted_label = le.inverse_transform([val_preds[i]])[0]\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "_P-58Yir_Izv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revised Testing"
      ],
      "metadata": {
        "id": "mFB1FbLG8Rze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_precision, test_recall, test_f1, test_accuracy, test_conf_matrix, test_labels, test_preds = evaluate(test_loader)\n",
        "print(f'Test Precision: {test_precision}, Recall: {test_recall}, F1-Score: {test_f1}, Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "QAwPoypv8UHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting"
      ],
      "metadata": {
        "id": "WKaGeG3y_OQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix for test set\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='coolwarm')\n",
        "plt.xlabel('Test Predicted Labels')\n",
        "plt.ylabel('Test True Labels')\n",
        "plt.title('Test Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YXV2nf_-_Cbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print some example predictions for test set\n",
        "print(\"\\nSample Predictions for Test Set:\")\n",
        "for i in range(100):\n",
        "    true_label = le.inverse_transform([test_labels[i]])[0]\n",
        "    predicted_label = le.inverse_transform([test_preds[i]])[0]\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "C8XHwQs8_Mvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with Case Tagging Features"
      ],
      "metadata": {
        "id": "RLRIapVaS0qS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_data_path = '/content/feature_extracted_data.csv'\n",
        "test_data_path = '/content/feature_extracted_dataTEST.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Define the dataset class\n",
        "class NLPDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.word_encoder = LabelEncoder().fit(data['Word'])\n",
        "        self.pos_encoder = LabelEncoder().fit(data['POS'])\n",
        "        self.lemma_encoder = LabelEncoder().fit(data['LEMMA'])\n",
        "        self.tag_encoder = LabelEncoder().fit(data['Tag'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.word_encoder.transform([self.data.iloc[idx]['Word']])[0]\n",
        "        pos = self.pos_encoder.transform([self.data.iloc[idx]['POS']])[0]\n",
        "        lemma = self.lemma_encoder.transform([self.data.iloc[idx]['LEMMA']])[0]\n",
        "        number = self.data.iloc[idx]['NUMBER']\n",
        "        contains_number = self.data.iloc[idx]['Contains NUMBER']\n",
        "        maj_number = self.data.iloc[idx]['Maj NUMBER']\n",
        "        tag = self.tag_encoder.transform([self.data.iloc[idx]['Tag']])[0]\n",
        "        return {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'number': torch.tensor(number, dtype=torch.float),\n",
        "            'contains_number': torch.tensor(contains_number, dtype=torch.float),\n",
        "            'maj_number': torch.tensor(maj_number, dtype=torch.float),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = NLPDataset(train_data)\n",
        "test_dataset = NLPDataset(test_data)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "m0WkdMgGS3Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class NLPModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim):\n",
        "        super(NLPModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.lemma_embeddings = nn.Embedding(num_embeddings_lemma, lemma_embedding_dim)\n",
        "\n",
        "        # Feature weight\n",
        "        self.feature_weight = 0.1  # To reduce their impact\n",
        "\n",
        "        # Concatenation dimension\n",
        "        concat_dim = embedding_dim + pos_embedding_dim + lemma_embedding_dim + 3  # 3 for the additional features\n",
        "\n",
        "        # Dense layer prior to LSTM\n",
        "        self.dense = nn.Linear(concat_dim, hidden_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_out_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(2 * lstm_out_dim, output_dim)\n",
        "\n",
        "    def forward(self, word_input, pos_input, lemma_input, number, contains_number, maj_number):\n",
        "        word_embeds = self.word_embeddings(word_input)\n",
        "        pos_embeds = self.pos_embeddings(pos_input)\n",
        "        lemma_embeds = self.lemma_embeddings(lemma_input)\n",
        "\n",
        "        # Apply weight to number features\n",
        "        number = number * self.feature_weight\n",
        "        contains_number = contains_number * self.feature_weight\n",
        "        maj_number = maj_number * self.feature_weight\n",
        "\n",
        "        # Concatenate all features\n",
        "        combined = torch.cat((word_embeds, pos_embeds, lemma_embeds, number.unsqueeze(-1), contains_number.unsqueeze(-1), maj_number.unsqueeze(-1)), dim=-1)\n",
        "\n",
        "        # Dense layer\n",
        "        dense_out = self.dense(combined)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(dense_out)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(lstm_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "okf-yFLeTHKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, criterion, and optimizer\n",
        "model = NLPModel(num_embeddings=len(train_dataset.word_encoder.classes_),\n",
        "                 embedding_dim=100,\n",
        "                 num_pos_tags=len(train_dataset.pos_encoder.classes_),\n",
        "                 pos_embedding_dim=25,\n",
        "                 num_embeddings_lemma=len(train_dataset.lemma_encoder.classes_),\n",
        "                 lemma_embedding_dim=25,\n",
        "                 hidden_dim=128,\n",
        "                 lstm_out_dim=64,\n",
        "                 output_dim=len(train_dataset.tag_encoder.classes_))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with progress bar\n",
        "num_epochs = 10\n",
        "batch_size = 16  # Reduced batch size for more frequent updates\n",
        "from tqdm import tqdm  # Progress bar library\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    num_batches = len(train_dataloader)\n",
        "\n",
        "    # Using tqdm for the progress bar\n",
        "    with tqdm(total=num_batches, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            words = batch['word']\n",
        "            pos = batch['pos']\n",
        "            lemma = batch['lemma']\n",
        "            number = batch['number']\n",
        "            contains_number = batch['contains_number']\n",
        "            maj_number = batch['maj_number']\n",
        "            tags = batch['tag']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(words, pos, lemma, number, contains_number, maj_number)\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({'Loss': epoch_loss / (batch_idx + 1)})\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / num_batches}')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        number = batch['number']\n",
        "        contains_number = batch['contains_number']\n",
        "        maj_number = batch['maj_number']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        outputs = model(words, pos, lemma, number, contains_number, maj_number)\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, -1)\n",
        "        correct_predictions += (predicted == tags).sum().item()\n",
        "        total_predictions += tags.numel()\n",
        "\n",
        "        all_test_preds.extend(predicted.cpu().numpy().flatten())\n",
        "        all_test_labels.extend(tags.cpu().numpy().flatten())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_dataloader)\n",
        "test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}')\n",
        "print(f'Test Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSC2dAXJTJO8",
        "outputId": "d1fa247a-7762-4557-ad16-ffca8fbe28f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  11%|█▏        | 599/5278 [12:55<1:35:48,  1.23s/batch, Loss=0.394]"
          ]
        }
      ]
    }
  ]
}