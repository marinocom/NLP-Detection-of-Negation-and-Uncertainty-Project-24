{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "UpMHkIhmWTYH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame from the provided CSV\n",
        "df = pd.read_csv('/content/feature_extracted_data.csv')\n",
        "print(\"Column Names:\", df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g76qjMYqatEV",
        "outputId": "1877ac09-b0ec-4279-d0b5-f829c91a780d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Names: Index(['Word', 'Initial Scopes', 'Final Scopes', 'Tag', 'POS', 'LEMMA',\n",
            "       'NUMBER', 'Contains NUMBER', 'Maj NUMBER', 'text_id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaQcxotWNvj",
        "outputId": "164bec47-156b-4082-e796-74e6139318f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Columns:        Word  Word_idx    POS  POS_idx     LEMMA  Lemma_idx Tag  Tag_idx\n",
            "0  paciente     21226   NOUN        7  paciente      10546   O        2\n",
            "1       que     23115  SCONJ       14       que      11595   O        2\n",
            "2   ingresa     17572   VERB       16  ingresar       8590   O        2\n",
            "3        de     12698    ADP        1        de       5845   O        2\n",
            "4     forma     15630   NOUN        7     forma       7489   O        2\n"
          ]
        }
      ],
      "source": [
        "# Encoding categorical columns\n",
        "word_encoder = LabelEncoder()\n",
        "pos_encoder = LabelEncoder()\n",
        "lemma_encoder = LabelEncoder()\n",
        "tag_encoder = LabelEncoder()\n",
        "\n",
        "df['Word_idx'] = word_encoder.fit_transform(df['Word'])\n",
        "df['POS_idx'] = pos_encoder.fit_transform(df['POS'])\n",
        "df['Lemma_idx'] = lemma_encoder.fit_transform(df['LEMMA'])\n",
        "df['Tag_idx'] = tag_encoder.fit_transform(df['Tag'])\n",
        "\n",
        "# Verify new columns\n",
        "print(\"Encoded Columns:\", df[['Word', 'Word_idx', 'POS', 'POS_idx', 'LEMMA', 'Lemma_idx', 'Tag', 'Tag_idx']].head())\n",
        "\n",
        "# Custom Dataset class\n",
        "class NLPCustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.dataframe.iloc[idx]['Word_idx']\n",
        "        pos = self.dataframe.iloc[idx]['POS_idx']\n",
        "        lemma = self.dataframe.iloc[idx]['Lemma_idx']\n",
        "        tag = self.dataframe.iloc[idx]['Tag_idx']\n",
        "\n",
        "        sample = {\n",
        "            'word': torch.tensor(word, dtype=torch.long),\n",
        "            'pos': torch.tensor(pos, dtype=torch.long),\n",
        "            'lemma': torch.tensor(lemma, dtype=torch.long),\n",
        "            'tag': torch.tensor(tag, dtype=torch.long)\n",
        "        }\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NLPModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_pos_tags, pos_embedding_dim, num_embeddings_lemma, lemma_embedding_dim, hidden_dim, lstm_out_dim, output_dim):\n",
        "        super(NLPModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)  # Pre-trained, frozen\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.lemma_embedding = nn.Embedding(num_embeddings_lemma, lemma_embedding_dim)\n",
        "\n",
        "        # Concatenation dimension\n",
        "        concat_dim = embedding_dim + pos_embedding_dim + lemma_embedding_dim\n",
        "\n",
        "        # Dense layer prior to LSTM\n",
        "        self.dense = nn.Linear(concat_dim, hidden_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_out_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(2 * lstm_out_dim, output_dim)  # Correct input dimension\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x, pos, lemma):\n",
        "        # Embed each input type\n",
        "        x = self.word_embeddings(x)\n",
        "        pos = self.pos_embeddings(pos)\n",
        "        lemma = self.lemma_embedding(lemma)  # Corrected from 'case' to 'lemma'\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat((x, pos, lemma), dim=-1)\n",
        "\n",
        "        # Apply dense and activation\n",
        "        x = torch.tanh(self.dense(x))\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(lstm_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "8F8TFkvnWaqf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating Dataset and DataLoader for training and validation\n",
        "train_dataset = NLPCustomDataset(train_df)\n",
        "val_dataset = NLPCustomDataset(val_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initializing the model\n",
        "model = NLPModel(\n",
        "    num_embeddings=len(word_encoder.classes_),\n",
        "    embedding_dim=50,\n",
        "    num_pos_tags=len(pos_encoder.classes_),\n",
        "    pos_embedding_dim=10,\n",
        "    num_embeddings_lemma=len(lemma_encoder.classes_),\n",
        "    lemma_embedding_dim=10,\n",
        "    hidden_dim=100,\n",
        "    lstm_out_dim=50,\n",
        "    output_dim=len(tag_encoder.classes_)\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPjyYXB_YZ3d",
        "outputId": "6c0d64bb-7fec-421c-b855-da7443c67284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLPModel(\n",
            "  (word_embeddings): Embedding(27100, 50)\n",
            "  (pos_embeddings): Embedding(18, 10)\n",
            "  (lemma_embedding): Embedding(13927, 10)\n",
            "  (dense): Linear(in_features=70, out_features=100, bias=True)\n",
            "  (lstm): LSTM(100, 50, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        words = batch['word']\n",
        "        pos = batch['pos']\n",
        "        lemma = batch['lemma']\n",
        "        tags = batch['tag']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words, pos, lemma)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "    # Validation step (optional)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            words = batch['word']\n",
        "            pos = batch['pos']\n",
        "            lemma = batch['lemma']\n",
        "            tags = batch['tag']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(words, pos, lemma)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), tags.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, -1)\n",
        "            correct_predictions += (predicted == tags).sum().item()\n",
        "            total_predictions += tags.numel()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7hA2VjoXpmQ",
        "outputId": "27ac280c-2680-4519-c900-04fd8fe12b10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3359713606596374\n",
            "Epoch 2/10, Loss: 0.26924997499720293\n",
            "Epoch 3/10, Loss: 0.2444174414001742\n",
            "Epoch 4/10, Loss: 0.22818499560115704\n",
            "Epoch 5/10, Loss: 0.2168491954915005\n",
            "Epoch 6/10, Loss: 0.2084509050277294\n",
            "Epoch 7/10, Loss: 0.20088725881273928\n",
            "Epoch 8/10, Loss: 0.19471634228048013\n",
            "Epoch 9/10, Loss: 0.18973186892694824\n",
            "Epoch 10/10, Loss: 0.18609441715404446\n"
          ]
        }
      ]
    }
  ]
}